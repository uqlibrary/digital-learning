{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZQN1EXref5HUbmm9/Xwiy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WFSQ5E-l-pC"
      },
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "!pip install pandas numpy openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your CSV file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "4-BEeBBisrf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from openpyxl import Workbook\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "pd.set_option('display.max_columns', 39)"
      ],
      "metadata": {
        "id": "1rMkpKqTsxDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions\n",
        "def create_a_filter(df, column, StringIncluded):\n",
        "    return df[column].str.contains(StringIncluded, na=False, case=False)\n",
        "\n",
        "def add_comment_to_flagged_items(check_items, comment):\n",
        "    check_items['CKEY'] = check_items['CKEY'].fillna(comment)\n",
        "\n",
        "def create_a_string_length_filter(df, column, desired_length, str_to_replace='; '):\n",
        "    return df[column].str.replace(str_to_replace, '', regex=False).str.replace(u'\\u200e', '', regex=False).replace(np.nan, 'x' * desired_length).apply(len).apply(lambda x: x % desired_length == 0)\n",
        "\n",
        "def flag_items(df, column, string_values, comment, optional_ignore_str=False):\n",
        "    if optional_ignore_str:\n",
        "        item_filter = create_a_filter(df, column, string_values) & ~create_a_filter(df, column, optional_ignore_str)\n",
        "    else:\n",
        "        item_filter = create_a_filter(df, column, string_values)\n",
        "    items_to_check = df[item_filter].copy()\n",
        "    add_comment_to_flagged_items(items_to_check, comment)\n",
        "    return items_to_check\n",
        "\n",
        "def flag_incorrect_length(df, column, length, comment, str_to_replace='; '):\n",
        "    items_filter = create_a_string_length_filter(df, column, length, str_to_replace)\n",
        "    items_check = df[~items_filter].copy()\n",
        "    add_comment_to_flagged_items(items_check, comment)\n",
        "    return items_check\n",
        "\n",
        "def flag_items_with_2_strings(df, column, first_string, second_string, comment):\n",
        "    item_filter = create_a_filter(df, column, first_string) & create_a_filter(df, column, second_string)\n",
        "    items_to_check = df[item_filter].copy()\n",
        "    add_comment_to_flagged_items(items_to_check, comment)\n",
        "    return items_to_check\n",
        "\n",
        "def flag_missing_data(df, column, comment, includedtype=False, excludedType=False):\n",
        "    if includedtype:\n",
        "        items_filter = df[column].isna() & df['Type'].isin(includedtype)\n",
        "    elif excludedType:\n",
        "        items_filter = df[column].isna() & ~df['Type'].isin(excludedType)\n",
        "    else:\n",
        "        items_filter = df[column].isna()\n",
        "    items_check = df.loc[items_filter].copy()\n",
        "    add_comment_to_flagged_items(items_check, comment)\n",
        "    return items_check\n",
        "\n",
        "def create_the_df(file_name):\n",
        "    df = pd.read_csv(file_name,\n",
        "                     dtype={'ISBN10': str, 'ISBN13': str, 'ISSN': str, 'EISSN': str, 'LCN': str,'Library Note': str, 'Student Note': str, 'Importance': str},\n",
        "                     parse_dates=['Date Added', 'Last Published'])\n",
        "    df = df.sort_values(by=[\"List Appearance\"])\n",
        "    return df\n",
        "\n",
        "def remove_library_and_training_lists(df, unnecessary_lists):\n",
        "    unnecessary_lists_filter = create_a_filter(df, 'List Appearance', unnecessary_lists)\n",
        "    df.drop(index=df[unnecessary_lists_filter].index, inplace=True)\n",
        "    return df\n",
        "\n",
        "def create_academic_bookmarks_df(df, staff_names):\n",
        "    academic_filter = df['Added By'].isin(staff_names)\n",
        "    academic_df = df.loc[~academic_filter].copy()\n",
        "    academic_df.dropna(subset=\"Added By\", inplace=True)\n",
        "    return academic_df\n",
        "\n",
        "def create_paragraphs_df(df):\n",
        "    paragraph_filter = df.dropna(how='all', subset=['Title', 'Chapter/Article Title', 'Author(s)', 'Type'])\n",
        "    return df[~df.index.isin(paragraph_filter.index)]\n",
        "\n",
        "def remove_paragraph_items(df):\n",
        "    return df.dropna(how='all', subset=['Title', 'Chapter/Article Title', 'Author(s)', 'Type'])\n",
        "\n",
        "def create_archives_df(df):\n",
        "    return df.loc[create_a_filter(df, \"Web Address\", \"https://archive.org\")]\n",
        "\n",
        "def create_TADCS_df(df):\n",
        "    return df.loc[df['TADC Request Status'].isin(['LIVE', 'REFERRED', 'EXPIRED'])]\n",
        "\n",
        "def create_urls_and_DOIs_df(df):\n",
        "    return df.dropna(how='all', subset=['DOI', 'Web Address', 'Primary Web Address', 'Secondary Web Address'])\n",
        "\n",
        "def flag_rejected_withdrawn_TADCS(df):\n",
        "    TADCs = df.dropna(subset=['TADC Request Status'])\n",
        "    TADC_check_df = TADCs.loc[~TADCs['TADC Request Status'].isin(['LIVE', 'REFERRED', 'EXPIRED'])].copy()\n",
        "    add_comment_to_flagged_items(TADC_check_df, \"Checked rejected TADC\")\n",
        "    return TADC_check_df\n",
        "\n",
        "def sort_URLS_from_longest_to_shortest(df):\n",
        "    url_length = df['Web Address'].str.split('; ').replace(np.nan, 'Missing').apply(lambda x: max(x, key=len)).apply(len)\n",
        "    df[\"URL Length\"] = url_length\n",
        "    return df.sort_values(\"URL Length\", ascending=False)\n",
        "\n",
        "def flag_print_chapters_articles(print_df):\n",
        "    TADC_missing_filt = print_df[\"Type\"].isin(['Chapter', 'Article'])\n",
        "    TADC_missing_check = print_df.loc[TADC_missing_filt].copy()\n",
        "    add_comment_to_flagged_items(TADC_missing_check, \"Missing TADC?\")\n",
        "    return TADC_missing_check\n",
        "\n",
        "def flag_bad_urls(df):\n",
        "    bad_urls = [\"summon\", \"primo\", \"coursebank\", \"UQL.eblib\", \"learn.uq\", \"proxy.openathens\", \"ezproxy\"]\n",
        "    items_to_check = [flag_items(df, \"Web Address\", x, f\"{x} URL\") for x in bad_urls]\n",
        "    alma_urls_check = flag_items(df, \"Web Address\", \"alma\", \"alma URL\", optional_ignore_str=\"permalink\")\n",
        "    ebsco_urls_check = flag_items(df, \"Web Address\", \"ebsco\", \"Incorrect EBSCO URL\", optional_ignore_str=\"custid=s1097571|search.ebscohost.com%2Flogin.aspx%3Fauthtype%3Dip%2Cuid%26profile%3Dbsi\")\n",
        "    gruyter_urls_check = flag_items_with_2_strings(df, \"Web Address\", \"gruyter\", \"/product/\", \"incorrect De Gruyter URL\")\n",
        "    espace_urls_check = flag_items_with_2_strings(df, \"Web Address\", \"espace\", \"pdf\", \"espace pdf URL\")\n",
        "    return pd.concat(items_to_check + [alma_urls_check, ebsco_urls_check, gruyter_urls_check, espace_urls_check])\n",
        "\n",
        "def flag_bad_DOIS(df, column, comment):\n",
        "    items_filt = (~df[column].str.startswith(\"10.\", na=False)) & (df[column].notna())\n",
        "    items_to_check = df.loc[items_filt].copy()\n",
        "    add_comment_to_flagged_items(items_to_check, comment)\n",
        "    return items_to_check\n",
        "\n",
        "def check_for_missing_auth(cell, vendors, delim=\"; \"):\n",
        "    if isinstance(cell, str):\n",
        "        cell_list = cell.lower().split(delim)\n",
        "        return any(any(v in x for v in vendors) and \"openathens\" not in x and \"resolver.library\" not in x for x in cell_list)\n",
        "    return False\n",
        "\n",
        "def flag_missing_auth(df, column, comment, vendors):\n",
        "    item_filter = df[column].apply(lambda x: check_for_missing_auth(x, vendors))\n",
        "    items_to_check = df[item_filter].copy()\n",
        "    add_comment_to_flagged_items(items_to_check, comment)\n",
        "    return items_to_check\n",
        "\n",
        "def create_vendors_df(df, column, vendors):\n",
        "    pattern = '|'.join(vendors) + \"|ebscohost|proquest\"\n",
        "    item_filter = df[column].str.contains(pattern, case=False, na=False)\n",
        "    vendor_urls = df[item_filter].copy()\n",
        "    dois = df.loc[df[\"DOI\"].notna()]\n",
        "    return pd.concat([vendor_urls, dois]).drop_duplicates()\n",
        "\n",
        "def create_no_view_online_df(df, comment):\n",
        "    view_online_filter = df['Online Resource Source'].isna() & df['Importance'].isin([\"Recommended\", \"Further\"]) & ~df['TADC Request Status'].isin(['LIVE', 'REFERRED', 'EXPIRED']) & ~create_a_filter(df, \"Web Address\", \"https://archive.org\")\n",
        "    view_online_check = df.loc[view_online_filter].copy()\n",
        "    add_comment_to_flagged_items(view_online_check, comment)\n",
        "    return view_online_check\n",
        "\n",
        "def flag_open_url(df, column, comment):\n",
        "    item_filter = df[column] == \"Open Url\"\n",
        "    items_to_check = df[item_filter].copy()\n",
        "    add_comment_to_flagged_items(items_to_check, comment)\n",
        "    return items_to_check\n",
        "\n",
        "def flag_author_name_format(df, column, comment):\n",
        "    item_filter = df[column].str.contains(\",\", na=False) & ~df[column].str.contains(\";\", na=False)\n",
        "    items_to_check = df[item_filter].copy()\n",
        "    add_comment_to_flagged_items(items_to_check, comment)\n",
        "    return items_to_check"
      ],
      "metadata": {
        "id": "qcMMP_ZIs1c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Main Execution ===\n",
        "file_name = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "ARc-5jt2tGTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your variables\n",
        "unnecessary_lists = ' test| Library| Resources| Toolkit|TEST0018'\n",
        "staff_names = ['Dowe,Rachel', 'Buteeva,Rita', 'Deutsch,Corinna', 'Hull,Natalie', 'Owens,Kia', 'Lau,Ella', 'McDouall,Alex']\n",
        "vendors = [\"oxford\", \"degruyter\", \"wiley\", \"jstor\", \"springer\", \"muse.jhu\", \"duke\", \"informit\", \"springer\", \"link.gale\", \"clinicalkey\", \"academic.oup\", \"brill\", \"sagepub\", \"fulcrum\", \"tandfonline\", \"loebclassics\", \"cabidigital\", \"taylorfrancis\", \"bloomsbury\", \"doi.org\", \"ovid\", \"factiva\", \"austlit\", \"sciencedirect\", \"vitalsource\", \"heinonline\", \"manchesterhive\", \"knovel\", \"alexanderstreet\", \"r2library\"]\n"
      ],
      "metadata": {
        "id": "CesQBJJ54D-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and process the data\n",
        "df = create_the_df(file_name)\n",
        "df = remove_library_and_training_lists(df, unnecessary_lists)\n",
        "academic_df = create_academic_bookmarks_df(df, staff_names)\n",
        "paragraph_df = create_paragraphs_df(df)\n",
        "df = remove_paragraph_items(df)\n",
        "archive_df = create_archives_df(df)\n",
        "TADCS_df = create_TADCS_df(df)\n",
        "urls_and_DOIs_df = create_urls_and_DOIs_df(df)\n",
        "online_df = pd.concat([TADCS_df, urls_and_DOIs_df], axis='rows', sort=False).drop_duplicates()\n",
        "print_df = df[~df.index.isin(online_df.index)]\n",
        "vendors_df = create_vendors_df(urls_and_DOIs_df, \"Web Address\", vendors)\n",
        "url_length_df = sort_URLS_from_longest_to_shortest(df)\n",
        "df = df.drop(columns=[\"URL Length\"])\n",
        "view_online_button = create_no_view_online_df(urls_and_DOIs_df, \"View Online Button?\")"
      ],
      "metadata": {
        "id": "w4vWRKI45RAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flag issues\n",
        "check_dataframes = [\n",
        "      flag_missing_auth(urls_and_DOIs_df, \"Web Address\", \"Missing openathen auth\", vendors),\n",
        "      flag_missing_data(vendors_df, \"LCN\", \"Add LCN to eBook?\", includedtype=[\"Book\"]),\n",
        "      flag_missing_data(print_df, \"LCN\", \"No access - Missing LCN/URL\"),\n",
        "      flag_print_chapters_articles(print_df),\n",
        "      flag_missing_data(df, \"Date of Publication\", \"Date of pub\", excludedType=['Webpage', 'Website', 'Journal']),\n",
        "      flag_items(archive_df, \"Online Resource Web Address\", \"archive.org\", \"Check archive.org access\"),\n",
        "      flag_items(print_df, \"Publisher\", \"springer\", \"Check for Springer ebook\"),\n",
        "      flag_bad_urls(df),\n",
        "      flag_items(df, 'Online Resource Web Address', 'ezproxy', \"View Online button glitching\"),\n",
        "      flag_rejected_withdrawn_TADCS(df),\n",
        "      flag_bad_DOIS(df, \"DOI\", \"Check DOI\"),\n",
        "      flag_incorrect_length(df, \"ISBN10\", 10, \"ISBN10\"),\n",
        "      flag_incorrect_length(df, \"ISBN13\", 13, \"ISBN13\"),\n",
        "      flag_incorrect_length(df.assign(checking=df[\"ISSN\"].str.replace(\"-\", \"\").str.replace(\"–\", \"\")), \"checking\", 8, \"ISSN\"),\n",
        "      flag_incorrect_length(df.assign(checking=df[\"EISSN\"].str.replace(\"-\", \"\").str.replace(\"–\", \"\")), \"checking\", 8, \"EISSN\"),\n",
        "      pd.concat([\n",
        "          flag_incorrect_length(df, \"LCN\", 18, \"LCN\"),\n",
        "          flag_items(df, \"LCN\", \";\", \"2 LCNS\")\n",
        "      ]).drop_duplicates(subset=\"Item Link\"),\n",
        "      flag_items(df, \"Student Note\", \"requested|Note for students|blackboard\", \"Check Student Note\"),\n",
        "      flag_items(df, \"Library Note\", \"read |can |help|please|pls|blackboard\", \"Check Library Note\", optional_ignore_str=\"You can remove this|We have added|we have ordered|please delete this bookmark if\"),\n",
        "      flag_open_url(df, \"Online Resource Source\", \"Default Open URL\"),\n",
        "      flag_author_name_format (df, \"Author(s)\", \"Check Author Field\")\n",
        "]"
      ],
      "metadata": {
        "id": "PaVepTFC5VMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all flagged items\n",
        "check_items_df = pd.concat(check_dataframes, sort=False)"
      ],
      "metadata": {
        "id": "ll4CK5Ru5t9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create Excel workbook\n",
        "def create_multiple_sheets_excel(dataframes_to_export, excel_sheet_titles):\n",
        "    wb = Workbook()\n",
        "    for d, s in zip(dataframes_to_export, excel_sheet_titles):\n",
        "        ws = wb.create_sheet(title=s)\n",
        "        for r in dataframe_to_rows(d, index=False, header=True):\n",
        "            ws.append(r)\n",
        "    if 'Sheet' in wb.sheetnames:\n",
        "        wb.remove(wb['Sheet'])\n",
        "    return wb"
      ],
      "metadata": {
        "id": "mIdYHrDq5z8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save workbook\n",
        "output_filename = \"output.xlsx\"\n",
        "workbook = create_multiple_sheets_excel(\n",
        "    [check_items_df, df, paragraph_df, url_length_df, view_online_button],\n",
        "    [\"Flagged\", \"Format Check\", \"Paragraphs\", \"URL Lengths\", \"View Online Button\"]\n",
        ")\n",
        "workbook.save(output_filename)"
      ],
      "metadata": {
        "id": "BOf1OWDw6TNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wVolp4X66avB",
        "outputId": "01eff649-ff43-40d1-8196-17a67120f408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bf1d3364-fec6-4fdf-a122-daa3ba17195b\", \"output.xlsx\", 108280)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}